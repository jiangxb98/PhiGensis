<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>4D Driving Scene Generation With Stereo Forcing</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">4D Driving Scene Generation With Stereo Forcing</h1>
            <!-- <h1 class="title is-2 publication-title">[TPAMI]</h1> -->
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=OrbGCGkAAAAJ" target="_blank">Hao Lu<sup>1</sup></a><sup>*</sup>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Zhuang Ma<sup>2</sup></a><sup>*</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=-ocVCHgAAAAJ&hl=zh-CN" target="_blank">Guangfeng Jiang<sup>3</sup></a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Wenhang Ge<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Bohan Li<sup>4</sup></a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Yuzhan Cai<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Wenzhao Zheng<sup>5</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN" target="_blank">Yunpeng Zhang<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=n7j4bJUAAAAJ&hl=zh-CN" target="_blank">Yingcong Chen<sup>1</sup><sup>†</sup></a>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Hong Kong University of Science and Technology (Guangzhou)</span>
              <span class="author-block"><sup>2</sup>PhiGent Robotics</span>
              <span class="author-block"><sup>3</sup>University of Science and Technology of China</span>
              <span class="author-block"><sup>4</sup>Shanghai Jiao Tong University</span>
              <span class="author-block"><sup>5</sup>University of California, Berkeley</span>
              <!-- <br>Conferance name and year</span> -->
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution <sup>†</sup> Corresponding Author </small></span>
            </div>
                  
            <!-- Arxiv PDF link -->
            <div class="column has-text-centered">
              <div class="publication-links">
                    
                <span class="link-block">
                  <!-- 这里添加arxiv url -->
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary PDF link -->
                <!-- 
              <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supplementary</span>
              </a>
            </span>
            -->

              <!-- Github link -->
            <span class="link-block">
                <a href="https://github.com/LuPaoPao/PhiGensis" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <!-- ArXiv abstract Link -->
            <!--
            <span class="link-block">
              <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
            </span>
            -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!--
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
-->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present <b>PhiGenesis</b>, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Overview -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <img src="static/images/Fig_ppl.png" alt="Overview pipeline"/>
        <div class="content has-text-justified">
          <p>
            <b>Overall framework of the proposed PhiGenesis.</b> (1) Stage 1 aims to train a 4D reconstrucion generalist. Multi-view images are first fed into a fixed, pre-trained video VAE. The multi-scale features extracted from the decoder of the video VAE are then passed through a range-view adapter to reconstruct the complete 4D scene (including optical flow, etc.). (2) The objective of Stage 2 is to enhance geometric consistency generation. We project the 4D scenes reconstructed based on history onto the future trajectory perspective. The rendered video is denoised according to geometric uncertainty by stereo forcing and then sent to the pre-trained encoder to obtain the rendered multi-view latent. The rendered multi-view latent and noise map are fed into the multi-view video diffusion model to generate the latent of the multi-view video of the target trajectory. The latent of multi-view video is fed into the pre-trained video decoder and the GS adapter of range-view to generate the 4D scene corresponding to the target trajectory.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Overview -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results</h2>
        <img src="static/images/table_recon.png" alt="Results"/>
        <div class="content has-text-justified">
          <p>
            Comparison with state-of-the-art methods on the Waymo and nuScenes datasets. Metrics reported include Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Depth RMSE (D-RMSE), Learned Perceptual Image Patch Similarity (LPIPS), and Pearson Correlation Coefficient (PCC). Higher values are better for PSNR, SSIM, and PCC; lower values are better for D-RMSE and LPIPS.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- PhiGenesis video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Temporal Multi-View Video Generation</h2>
    </div>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/recon/waymo_3x3_video13.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/recon/waymo_3x3_video25.mp4"
              type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Novel View Synthesis</h2>
    </div>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <source src="static/videos/nvs/S1.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <source src="static/videos/nvs/S2.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <source src="static/videos/nvs/S3.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <source src="static/videos/nvs/S4.mp4"
              type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- PhiGenesis Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container ">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">More DiST-T Results</h2>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/DiST_T/DiST_T_supp_1.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <img src="static/images/DiST_T/DiST_T_supp_2.png" alt="MY ALT TEXT"/>
      </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->



<!-- DiST-T video -->


<!-- <section class="section hero">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">DiST-T: Zero-shot on Waymo</h2>
        <video poster="" id="video1" autoplay controls muted loop height="100%">
          <source src="static/videos/DiST_T_zs_waymo.mp4"
          type="video/mp4">
        </video>
        <div class="content has-text-justified">
          <p> The number of cameras and their installation positions in the Waymo dataset, as well as the image resolutions, are quite different from those in nuScenes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->



<!-- Paper poster -->
<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
-->


<!--BibTex citation -->
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{guo2025dist4d,
      &nbsp; title={4D Driving Scene Generation With Stereo Forcing},
      &nbsp; author={Hao Lu and Zhuang Ma and Guangfeng Jiang and Wenhang Ge and Bohan Li and Yuzhan Cai and Wenzhao Zheng and Yunpeng Zhang and Yingcong Chen},
      &nbsp; journal={arXiv preprint arXiv:xxxx},
      &nbsp; year={2025}
    }</code></pre>
  </div>
</section> -->
<!--End BibTex citation -->

<!--
  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
-->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>



